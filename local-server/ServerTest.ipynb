{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0855ebf6-d272-4dc0-a0af-d02d0f606b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97f0a0edb84494c8607b00f98888f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3cdcf876dc043248cb9ab03d07de03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe7af09f7bb4be6a0bb44169045d63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f718449d4b0401bac8a95aaa214a586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a4dae056944b7c89812d04dde12068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/786 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec7145ee4da4648b38962cb1f2dd047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/superdocs/lib/python3.11/site-packages/accelerate/utils/modeling.py:1365: UserWarning: Current model requires 939531264 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Trying to set a tensor of shape torch.Size([152064, 448]) in \"weight\" (which has shape torch.Size([152064, 3584])), this look incorrect.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m draft_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen2-0.5B-Instruct-MLX\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m draft_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(draft_model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m NEWLINE_THRESHOLD \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/superdocs/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/superdocs/lib/python3.11/site-packages/transformers/modeling_utils.py:3916\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3907\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3909\u001b[0m     (\n\u001b[1;32m   3910\u001b[0m         model,\n\u001b[1;32m   3911\u001b[0m         missing_keys,\n\u001b[1;32m   3912\u001b[0m         unexpected_keys,\n\u001b[1;32m   3913\u001b[0m         mismatched_keys,\n\u001b[1;32m   3914\u001b[0m         offload_index,\n\u001b[1;32m   3915\u001b[0m         error_msgs,\n\u001b[0;32m-> 3916\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[1;32m   3917\u001b[0m         model,\n\u001b[1;32m   3918\u001b[0m         state_dict,\n\u001b[1;32m   3919\u001b[0m         loaded_state_dict_keys,  \u001b[38;5;66;03m# XXX: rename?\u001b[39;00m\n\u001b[1;32m   3920\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3921\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3922\u001b[0m         ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[1;32m   3923\u001b[0m         sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[1;32m   3924\u001b[0m         _fast_init\u001b[38;5;241m=\u001b[39m_fast_init,\n\u001b[1;32m   3925\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[1;32m   3926\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   3927\u001b[0m         offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   3928\u001b[0m         offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[1;32m   3929\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m   3930\u001b[0m         hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[1;32m   3931\u001b[0m         keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   3932\u001b[0m         gguf_path\u001b[38;5;241m=\u001b[39mgguf_path,\n\u001b[1;32m   3933\u001b[0m     )\n\u001b[1;32m   3935\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3936\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/superdocs/lib/python3.11/site-packages/transformers/modeling_utils.py:4390\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4386\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4387\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4388\u001b[0m                 )\n\u001b[1;32m   4389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4390\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   4391\u001b[0m             model_to_load,\n\u001b[1;32m   4392\u001b[0m             state_dict,\n\u001b[1;32m   4393\u001b[0m             loaded_keys,\n\u001b[1;32m   4394\u001b[0m             start_prefix,\n\u001b[1;32m   4395\u001b[0m             expected_keys,\n\u001b[1;32m   4396\u001b[0m             device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   4397\u001b[0m             offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   4398\u001b[0m             offload_index\u001b[38;5;241m=\u001b[39moffload_index,\n\u001b[1;32m   4399\u001b[0m             state_dict_folder\u001b[38;5;241m=\u001b[39mstate_dict_folder,\n\u001b[1;32m   4400\u001b[0m             state_dict_index\u001b[38;5;241m=\u001b[39mstate_dict_index,\n\u001b[1;32m   4401\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   4402\u001b[0m             hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[1;32m   4403\u001b[0m             is_safetensors\u001b[38;5;241m=\u001b[39mis_safetensors,\n\u001b[1;32m   4404\u001b[0m             keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   4405\u001b[0m             unexpected_keys\u001b[38;5;241m=\u001b[39munexpected_keys,\n\u001b[1;32m   4406\u001b[0m         )\n\u001b[1;32m   4407\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4409\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/superdocs/lib/python3.11/site-packages/transformers/modeling_utils.py:936\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    925\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m is_quantized\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mrequires_parameters_quantization)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m ):\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 936\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mset_module_kwargs)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    938\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/superdocs/lib/python3.11/site-packages/accelerate/utils/modeling.py:358\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m old_value\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m value\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m--> 358\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    359\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrying to set a tensor of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (which has shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_value\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), this look incorrect.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    360\u001b[0m         )\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;66;03m# For compatibility with PyTorch load_state_dict which converts state dict dtype to existing dtype in model\u001b[39;00m\n\u001b[1;32m    364\u001b[0m         value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mto(old_value\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Trying to set a tensor of shape torch.Size([152064, 448]) in \"weight\" (which has shape torch.Size([152064, 3584])), this look incorrect."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import difflib\n",
    "from diff_utils import find_best_match, parse_diff\n",
    "from typing import List, Optional, Tuple\n",
    "from transformers.generation.candidate_generator import CandidateGenerator, _crop_past_key_values\n",
    "from transformers.generation.stopping_criteria import StoppingCriteria\n",
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "\n",
    "model_name = \"Qwen/Qwen2-7B-Instruct-MLX\"\n",
    "draft_model_name = \"Qwen/Qwen2-0.5B-Instruct-MLX\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\")\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(draft_model_name, trust_remote_code=True, device_map=\"auto\")\n",
    "\n",
    "NEWLINE_THRESHOLD = 10\n",
    "newline_token = tokenizer.encode(\"\"\"\n",
    "\"\"\")[-1]\n",
    "\n",
    "@torch.no_grad() # Pulled directly from the demo_pld notebook.\n",
    "def find_candidate_pred_tokens(input_ids, max_ngram_size=3, num_pred_tokens=10):\n",
    "    input_length = input_ids.size(1)\n",
    "    if max_ngram_size <= 0 or num_pred_tokens <= 0 or max_ngram_size > input_length:\n",
    "        raise ValueError(\"Invalid max_ngram_size or num_pred_tokens\")\n",
    "    for ngram_size in range(max_ngram_size, 0, -1):\n",
    "        ngram = input_ids[0, -ngram_size:].tolist()\n",
    "        windows = input_ids.unfold(dimension=1, size=ngram_size, step=1)\n",
    "        ngram_tensor = torch.tensor(ngram, device=input_ids.device).unsqueeze(0)\n",
    "        matches = (windows == ngram_tensor).all(dim=2)\n",
    "        match_indices = matches.nonzero(as_tuple=True)[1]\n",
    "        for idx in match_indices:\n",
    "            start_idx = idx + ngram_size\n",
    "            end_idx = start_idx + num_pred_tokens\n",
    "            if start_idx < input_length - ngram_size:\n",
    "                return input_ids[0, start_idx:min(end_idx, input_length)]\n",
    "    return torch.tensor([100], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_candidate_pred_tokens_diff(input_ids, code_ids, orig_input_len=0, ngram_size=3, num_pred_tokens=10):\n",
    "    # start_time = time.perf_counter()\n",
    "    input_length = input_ids.size(1)\n",
    "    code_length = len(code_ids)\n",
    "\n",
    "    # Ensure max_ngram_size and num_pred_tokens are valid\n",
    "    if ngram_size <= 0 or ngram_size > input_length:\n",
    "        raise ValueError(\"Invalid max_ngram_size or num_pred_tokens\")\n",
    "\n",
    "    sm = difflib.SequenceMatcher(None, code_ids, input_ids[0, orig_input_len:].tolist())\n",
    "    \n",
    "    deleted = added = changed = same = last_deleted = 0\n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == 'replace':\n",
    "            changed += i2 - i1\n",
    "        elif tag == 'delete':\n",
    "            deleted += i2 - i1\n",
    "            last_deleted = i2 - i1\n",
    "        elif tag == 'insert':\n",
    "            added += j2 - j1\n",
    "        elif tag == 'equal':\n",
    "            same += i2 - i1\n",
    "    \n",
    "    approx_tokens_original = changed + deleted + same - last_deleted\n",
    "\n",
    "    lookback_start = max(input_length - ngram_size, orig_input_len)\n",
    "    search_ngram = input_ids[0, lookback_start:].tolist()\n",
    "\n",
    "    for ngram_start in range(max(0, approx_tokens_original - ngram_size), len(code_ids)):\n",
    "        # if there is a match, return the entire rest of the tokens.\n",
    "        if ngram_start + len(search_ngram) >= len(code_ids):\n",
    "            break\n",
    "        if search_ngram == code_ids[ngram_start:ngram_start + len(search_ngram)]:\n",
    "            return torch.tensor(code_ids[ngram_start + len(search_ngram):max(ngram_start + len(search_ngram) + num_pred_tokens, len(code_ids))], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "    # If no match is found, return what the answer would be otherwise\n",
    "    # print(\"Diff searching took: \", time.perf_counter() - start_time)\n",
    "    return find_candidate_pred_tokens(input_ids, ngram_size, num_pred_tokens)\n",
    "    # return torch.tensor([], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "\n",
    "class DiffPromptLookupCandidateGenerator(CandidateGenerator):\n",
    "    def __init__(self, input_ids, code_ids, ngram_size=3, num_pred_tokens=10):\n",
    "        self.code_ids = code_ids\n",
    "        self.orig_input_len = input_ids.shape[-1]\n",
    "        self.ngram_size = ngram_size\n",
    "        self.num_pred_tokens = num_pred_tokens\n",
    "    \n",
    "    def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n",
    "        # print(\"Getting candidates\")\n",
    "        return torch.cat(\n",
    "            (\n",
    "                input_ids,\n",
    "                find_candidate_pred_tokens_diff(input_ids, self.code_ids, self.orig_input_len, self.ngram_size, self.num_pred_tokens).unsqueeze(0)\n",
    "            ),\n",
    "            dim=-1\n",
    "        ), None\n",
    "    \n",
    "    def update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, num_matches: int): # Maybe use the number of matches/scores to have a threshold\n",
    "        pass \n",
    "\n",
    "class NumRunsStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, max_num_runs=4):\n",
    "        self.max_num_runs = 4\n",
    "        self.num_runs = 0\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n",
    "        self.num_runs += 1\n",
    "        return self.num_runs >= self.max_num_runs\n",
    "\n",
    "def _get_default_candidate_generator_generator(generator: CandidateGenerator):\n",
    "    def _get_candidate_generator(self, **kwargs):\n",
    "        return generator\n",
    "    return _get_candidate_generator\n",
    "\n",
    "class TwoLayerLookupCandidateGenerator(CandidateGenerator):\n",
    "    def __init__(self, draft_model, input_ids, code_ids, num_runs=4, **diff_prompt_args):\n",
    "        self.draft_model = draft_model\n",
    "        self.input_ids = input_ids\n",
    "        self.code_ids = code_ids\n",
    "        self.candidate_generator = DiffPromptLookupCandidateGenerator(\n",
    "            self.input_ids, \n",
    "            self.code_ids,\n",
    "            **diff_prompt_args\n",
    "        )\n",
    "        \n",
    "        self.past_keys_values = None\n",
    "        self.num_runs = num_runs\n",
    "\n",
    "        self.draft_model._get_candidate_generator = (_get_default_candidate_generator_generator(self.candidate_generator)).__get__(self.draft_model, type(self.draft_model))\n",
    "\n",
    "    def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n",
    "        if self.past_keys_values:\n",
    "            self.past_keys_values = _crop_past_key_values(self.draft_model, self.past_keys_values, input_ids.shape[-1] - 1)\n",
    "        \n",
    "        generation = self.draft_model.generate(\n",
    "            inputs=input_ids,\n",
    "            prompt_lookup_num_tokens=1,\n",
    "            max_new_tokens=1000,\n",
    "            stopping_criteria=[NumRunsStoppingCriteria(self.num_runs)],\n",
    "            past_key_values=self.past_keys_values,\n",
    "            use_cache=True,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "\n",
    "        return generation.sequences, None\n",
    "\n",
    "    def update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, num_matches: int): # Maybe use the number of matches/scores to have a threshold\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33a1c8e-f185-4a18-be93-c4345e70ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Code:```python\\n{code_text}``` \\n\\n Question: {question} \\n\\n Modified code:\\n\".format(code_text=code_text, question=question)\n",
    "prompt = f\"# Code before:\\n{edit_request.file_content}\\n# Requested change:\\n{edit_request.query}\\n# Rewrite the code to incorporate the change:\\n```\\n\"\n",
    "code_inputs = tokenizer(edit_request.file_content, return_tensors=\"pt\").input_ids[0].tolist()\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "num_max_gen_tokens = inputs.shape[-1] + 300\n",
    "\n",
    "two_layer_candidate_generator = TwoLayerLookupCandidateGenerator(\n",
    "    draft_model,\n",
    "    inputs,\n",
    "    code_inputs,\n",
    "    ngram_size=5,\n",
    "    num_pred_tokens=50\n",
    ")\n",
    "model._get_candidate_generator = (_get_default_candidate_generator_generator(two_layer_candidate_generator)).__get__(model, type(model))\n",
    "\n",
    "test_out = model.generate(\n",
    "    inputs=inputs,\n",
    "    prompt_lookup_num_tokens=2,\n",
    "    max_new_tokens=num_max_gen_tokens,\n",
    "    use_cache=True,\n",
    "    streamer=TextStreamer(tokenizer)\n",
    ")\n",
    "\n",
    "text = tokenizer.batch_decode(test_out, skip_special_tokens=True)[0]\n",
    "text = text.split(\"```\")[1].split(\"```\")[0]\n",
    "\n",
    "unified_diff = \"\\n\".join(difflib.unified_diff(edit_request.file_content.splitlines(), text.splitlines(), n=3))\n",
    "search_replace_changes = parse_diff(unified_diff)\n",
    "\n",
    "fixed_file = edit_request.file_content\n",
    "for sr in search_replace_changes:\n",
    "    if len(sr.search_block.strip()) == 0:\n",
    "        continue\n",
    "    print(\"Search: \", sr.search_block)\n",
    "    sr.search_block = find_best_match(text, sr.search_block).block\n",
    "    print(\"Found block: \", sr.search_block)\n",
    "    print(\"Replace: \", sr.replace_block)\n",
    "    fixed_file = fixed_file.replace(sr.search_block, f\"\"\"<<<<<<< SEARCH\n",
    "{sr.search_block}\n",
    "=======\n",
    "{sr.replace_block}\n",
    ">>>>>>> REPLACE\"\"\")\n",
    "\n",
    "print(fixed_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
